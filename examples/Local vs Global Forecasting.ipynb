{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "924a0c0e-3578-4cdf-8a8e-b05c03160058",
   "metadata": {},
   "source": [
    "# Local vs Global Forecasting\n",
    "\n",
    "The related article is available [here](https://medium.com/towards-data-science/local-vs-global-forecasting-what-you-need-to-know-1cc29e66cae0).\n",
    "\n",
    "This notebook showcases the difference between the local and global approach for time-series forecasting. \n",
    "\n",
    "To install the needed dependencies, you can follow the instructions in the README at the root of the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cff910-5dfa-4bb6-a1f5-1570acea113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fce4051-1680-4f98-b469-4147f8e1e0ff",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "In this example we use the Australian Tourism dataset. \n",
    "\n",
    "The dataset is made of quarter time-series starting in 1998. In this notebook we consider the tourism numbers at a region level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a58bbe-3559-4da4-85c9-bf23720451c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data.\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/unit8co/darts/master/datasets/australian_tourism.csv')\n",
    "\n",
    "# Add time information: quarterly data starting in 1998.\n",
    "data.index = pd.date_range(\"1998-01-01\",periods = len(data), freq = \"3MS\")\n",
    "data.index.name = \"time\"\n",
    "\n",
    "# Consider only region-level data.\n",
    "data = data[['NSW','VIC', 'QLD', 'SA', 'WA', 'TAS', 'NT']]\n",
    "\n",
    "# Let's give it nicer names.\n",
    "data = data.rename(columns = {\n",
    "    'NSW': \"New South Wales\",\n",
    "    'VIC': \"Victoria\", \n",
    "    'QLD': \"Queensland\", \n",
    "    'SA': \"South Australia\", \n",
    "    'WA': \"Western Australia\", \n",
    "    'TAS': \"Tasmania\", \n",
    "    'NT': \"Northern Territory\",\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f902a551-4bc2-435f-9b54-42b9ce5f8cff",
   "metadata": {},
   "source": [
    "## Quick analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29201cb8-bae6-463b-a8ae-973aa7905697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the data.\n",
    "def show_data(data,title=\"\"):\n",
    "    trace = [go.Scatter(x=data.index,y=data[c],name=c) for c in data.columns]\n",
    "    go.Figure(trace,layout=dict(title=title)).show()\n",
    "\n",
    "show_data(data,\"Australian Tourism data by Region\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c17de9e3-ca21-4556-985d-4afdd260cc69",
   "metadata": {},
   "source": [
    "We can see that:\n",
    "- data exhibits a strong seasonality\n",
    "- the scale of the time-series is quite different across different regions\n",
    "- the length of the time-series is always the same\n",
    "- there's no missing data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0f31490-315e-4b38-a7d5-7a706e83bdc8",
   "metadata": {},
   "source": [
    "## Data Engineering\n",
    "\n",
    "Let's predict the value of the next quarter based on:\n",
    "- the lagged values of the previous 2 years.\n",
    "- the current quarter (as a categorical feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ecf712-e480-4d6e-93d6-9e722aa1431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_targets_features(data,lags=range(8),horizon=1):\n",
    "    features = {}\n",
    "    targets = {}\n",
    "    for c in data.columns:\n",
    "        \n",
    "        # Build lagged features.\n",
    "        feat = pd.concat([data[[c]].shift(lag).rename(columns = {c: f\"lag_{lag}\"}) for lag in lags],axis=1)\n",
    "        # Build quarter feature.\n",
    "        feat[\"quarter\"] = [f\"Q{int((m-1) / 3 + 1)}\" for m in data.index.month]\n",
    "        feat[\"quarter\"] = feat[\"quarter\"].astype(\"category\")\n",
    "\n",
    "        # Build target at horizon.\n",
    "        targ = data[c].shift(-horizon).rename(f\"horizon_{horizon}\")\n",
    "        \n",
    "        # Drop missing values generated by lags/horizon.\n",
    "        idx = ~(feat.isnull().any(axis=1) | targ.isnull())\n",
    "        features[c] = feat.loc[idx]\n",
    "        targets[c] = targ.loc[idx]\n",
    "        \n",
    "    return targets,features\n",
    "\n",
    "\n",
    "# Build targets and features.\n",
    "targets,features = build_targets_features(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0fd3f93e-f3d3-4570-a787-52c1b9aa741f",
   "metadata": {},
   "source": [
    "## Train/Test split\n",
    "\n",
    "Let's consider the last 2 years as test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f66d33e-3172-476d-bfed-8a7c7f24d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(targets,features,test_size=8):\n",
    "    targ_train = {k: v.iloc[:-test_size] for k,v in targets.items()}\n",
    "    feat_train = {k: v.iloc[:-test_size] for k,v in features.items()}\n",
    "    targ_test = {k: v.iloc[-test_size:] for k,v in targets.items()}\n",
    "    feat_test = {k: v.iloc[-test_size:] for k,v in features.items()}\n",
    "    return targ_train,feat_train,targ_test,feat_test\n",
    "\n",
    "targ_train,feat_train,targ_test,feat_test = train_test_split(targets,features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28f2d955-73da-43f6-844f-5ff536f003e9",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Now we estimate forecasting models using two different approaches:\n",
    "- Local Approach: estimate one model for each time-series\n",
    "- Global Approach: estimate one model for all time-series\n",
    "\n",
    "In both cases we use a LightGBM model with default parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2e752ce-b904-47aa-ae02-d1e719b7f1aa",
   "metadata": {},
   "source": [
    "\n",
    "### Local Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6594967f-86a3-4b61-82df-b21a2231ab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate one LightGBM model with default parameters for each target.\n",
    "local_models = {k: LGBMRegressor() for k in data.columns}\n",
    "\n",
    "# Fit the models on the training set.\n",
    "for k in data.columns:\n",
    "    local_models[k].fit(feat_train[k],targ_train[k])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1306476-179a-4f1d-b7e2-3df748ac7c29",
   "metadata": {},
   "source": [
    "### Global Approach\n",
    "\n",
    "The global approach needs a few extra steps. \n",
    "1. First, since the targets have different scales, we perform a normalization step. \n",
    "2. Then to allow the model to distinguish across different targets, we add a categorical feature for each target.\n",
    "3. Finally, we need to concatenate the data for different targets together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc62272b-5381-4476-a766-015b6e6ae30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_scalers(feat_train,targ_train):\n",
    "    feat_scalers = {k: MinMaxScaler().set_output(transform=\"pandas\") for k in feat_train}\n",
    "    targ_scalers = {k: MinMaxScaler().set_output(transform=\"pandas\") for k in feat_train}\n",
    "    for k in feat_train:\n",
    "        feat_scalers[k].fit(feat_train[k].drop(columns=\"quarter\"))\n",
    "        targ_scalers[k].fit(targ_train[k].to_frame())\n",
    "    return feat_scalers,targ_scalers\n",
    "        \n",
    "        \n",
    "def scale_features(feat,feat_scalers):\n",
    "    scaled_feat = {}\n",
    "    for k in feat:\n",
    "        df = feat[k].copy()\n",
    "        cols = [c for c in df.columns if c not in {\"quarter\"}]\n",
    "        df[cols] = feat_scalers[k].transform(df[cols])\n",
    "        scaled_feat[k] = df\n",
    "    return scaled_feat\n",
    "\n",
    "\n",
    "def scale_targets(targ,targ_scalers):\n",
    "    return {k: targ_scalers[k].transform(v.to_frame()) for k,v in targ.items()}\n",
    "\n",
    "\n",
    "# Fit scalers on numerical features and target on the training period.\n",
    "feat_scalers,targ_scalers = fit_scalers(feat_train,targ_train)\n",
    "\n",
    "# Scale train data.\n",
    "scaled_feat_train = scale_features(feat_train,feat_scalers)\n",
    "scaled_targ_train = scale_targets(targ_train,targ_scalers)\n",
    "\n",
    "# Scale test data.\n",
    "scaled_feat_test = scale_features(feat_test,feat_scalers)\n",
    "scaled_targ_test = scale_targets(targ_test,targ_scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd74338-e811-4973-aff0-c3b31e32d7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a `target_name` feature.\n",
    "def add_target_name_feature(feat):\n",
    "    for k,df in feat.items():\n",
    "        df[\"target_name\"] = k\n",
    "\n",
    "add_target_name_feature(scaled_feat_train)\n",
    "add_target_name_feature(scaled_feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f817b38d-6c94-4b3c-a33f-49c08de09039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the data.\n",
    "global_feat_train = pd.concat(scaled_feat_train.values())\n",
    "global_targ_train = pd.concat(scaled_targ_train.values())\n",
    "global_feat_test = pd.concat(scaled_feat_test.values())\n",
    "global_targ_test = pd.concat(scaled_targ_test.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b02ba2f-476a-42aa-8c29-1aa8da921715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make `target_name` categorical after concatenation.\n",
    "global_feat_train.target_name = global_feat_train.target_name.astype(\"category\")\n",
    "global_feat_test.target_name = global_feat_test.target_name.astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e4c47f-756b-4416-941e-5f59e867bea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a single LightGBM model with default parameters.\n",
    "global_model = LGBMRegressor()\n",
    "\n",
    "# Fit the models on the training set.\n",
    "global_model.fit(global_feat_train,global_targ_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "959b9de4-99f3-4518-afc0-29fb1907a6de",
   "metadata": {},
   "source": [
    "## Predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b059e1-ec77-49c1-ad3a-b502668bc995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the local models.\n",
    "pred_local = {k: model.predict(feat_test[k]) for k, model in local_models.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb5941-7672-4b9a-bab3-0a41bd5e47ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_global_model(global_model, global_feat_test, targ_scalers):\n",
    "    # Predict.\n",
    "    pred_global_scaled = global_model.predict(global_feat_test)\n",
    "    # Re-arrange the predictions\n",
    "    pred_df_global = global_feat_test[[\"target_name\"]].copy()\n",
    "    pred_df_global[\"predictions\"] = pred_global_scaled\n",
    "    pred_df_global = pred_df_global.pivot(\n",
    "        columns=\"target_name\", values=\"predictions\"\n",
    "    )\n",
    "    # Un-scale the predictions\n",
    "    return {\n",
    "        k: targ_scalers[k]\n",
    "        .inverse_transform(\n",
    "            pred_df_global[[k]].rename(\n",
    "                columns={k: global_targ_train.columns[0]}\n",
    "            )\n",
    "        )\n",
    "        .reshape(-1)\n",
    "        for k in pred_df_global.columns\n",
    "    }\n",
    "\n",
    "\n",
    "# Make predicitons with the global model.\n",
    "pred_global = predict_global_model(global_model, global_feat_test, targ_scalers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8f32c4c-1992-4069-bba8-77d35878bf24",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04b0c58-c22e-40c4-acfd-378cd672652e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "for k in targ_test:\n",
    "    df = targ_test[k].rename(\"target\").to_frame()\n",
    "    df[\"prediction_local\"] = pred_local[k]\n",
    "    df[\"prediction_global\"] = pred_global[k]\n",
    "    output[k] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b9add3-eeb9-4b0a-84a8-65ed6d9f892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(output):\n",
    "    output_all = pd.concat(output.values())\n",
    "    mae_local = (output_all.target - output_all.prediction_local).abs().mean()\n",
    "    mae_global = (output_all.target - output_all.prediction_global).abs().mean()\n",
    "\n",
    "    print(\"                            LOCAL     GLOBAL\")\n",
    "    print(f\"MAE overall              :  {mae_local:.1f}     {mae_global:.1f}\\n\")\n",
    "    for k,df in output.items():   \n",
    "        mae_local = (df.target - df.prediction_local).abs().mean()\n",
    "        mae_global = (df.target - df.prediction_global).abs().mean()\n",
    "        print(f\"MAE - {k:19}:  {mae_local:.1f}     {mae_global:.1f}\")\n",
    "\n",
    "# Let's show some statistics.\n",
    "print_stats(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da0bfda-bc4a-472d-a676-9be4d3aa6002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mae_by_timestep(output):\n",
    "    output_all = pd.concat(output.values())\n",
    "    df = pd.concat([\n",
    "            (output_all.target - output_all.prediction_local).abs().groupby(level=\"time\").mean().rename(\"mae_local\"),\n",
    "            (output_all.target - output_all.prediction_global).abs().groupby(level=\"time\").mean().rename(\"mae_global\"),\n",
    "        ], \n",
    "        axis=1,\n",
    "    )\n",
    "    show_data(df,\"MAE by timestep\")\n",
    "    \n",
    "# Show the mean absolute error per timestep.\n",
    "show_mae_by_timestep(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e831af5-90c4-4438-9770-504b7e10326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the predictions.\n",
    "for k,df in output.items():\n",
    "    show_data(df,k)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c42a329-35f6-44dd-85c1-166e863c6035",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "In this notebook we showcased the local and global approaches to time-series forecasting, using:\n",
    "- quarterly Australian tourism data\n",
    "- simple feature engineering\n",
    "- a LightGBM model with no hyperparameter tuning\n",
    "\n",
    "In this specific example, we saw that the global approach is superior, leading to a 43% lower mean absolute error than the local one.\n",
    "\n",
    "In particular, the global approach had a lower MAE on:\n",
    "- all the timesteps\n",
    "- all the targets except for Western Australia\n",
    "\n",
    "This was somehow expected, since:\n",
    "- We are predicting multiple correlated time-series\n",
    "- The depth of the historical data is very shallow\n",
    "- We are using a somehow comples model for such a shallow univariate time-series. A classical statistical model such as Exponential Smoothing might be more appropriate in this setting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f79079-1bcf-4d43-81ad-23274f5bc305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "effective-forecasting",
   "language": "python",
   "name": "effective-forecasting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
