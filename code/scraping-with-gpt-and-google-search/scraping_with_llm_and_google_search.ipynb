{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccc7577e-5549-43a3-9a44-eda2c60bd880",
   "metadata": {},
   "source": [
    "# Automatic Scraping with GPT and google search\n",
    "\n",
    "In this notebook we illustrate an example on how to perform automatic scraping with Google Search and GPT.\n",
    "\n",
    "In the notebooks we scrape data for publishing companies in Switzerland, but it can be easily extended to other use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc4ceb2-1be7-409f-a4f7-8f64add972a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib.parse import urlparse\n",
    "from urllib.robotparser import RobotFileParser\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from googlesearch import search\n",
    "from openai import BadRequestError, OpenAI\n",
    "from requests.exceptions import ConnectionError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93c6f20-0c07-4d90-b3fc-a2f9b0468602",
   "metadata": {},
   "source": [
    "## Input Parameters\n",
    "\n",
    "- `google_query`: The query to get a list of websites from google search. The resulting websites are used to identify a list of `target`.\n",
    "- `target`: A short description of the items we are looking for.\n",
    "- `fields`: A list of fields which we would like to fill for each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120778e6-3b29-4826-8b6e-a72c30f5fa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can adapt the inputs to your needs.\n",
    "# Note: you might need to add exception handling if you encounter an error.\n",
    "google_query = \"Switzerland list of book publishing companies\"\n",
    "target = \"publishing companies\"\n",
    "fields = [\n",
    "    \"genres\",\n",
    "    \"year founded\",\n",
    "    \"location\",\n",
    "    \"official website\",\n",
    "    \"short description\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578e0a2a-d068-4712-a6b5-12c29cce9903",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "Let's define some helper functions to use later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb53a18-6c0a-4058-99ed-801973fcc7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def can_scrape(url):\n",
    "    \"\"\"\n",
    "    Returns True if the useragent is allowed to fetch the url\n",
    "    according to the rules contained in the parsed robots.txt file.\n",
    "    \"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "    robots_url = f\"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\"\n",
    "    rp = RobotFileParser(robots_url)\n",
    "    rp.read()\n",
    "    return rp.can_fetch(\"*\", url)\n",
    "\n",
    "\n",
    "def fetch_html_content(search_results):\n",
    "    \"\"\"Get html for websites not forbidding scraping\"\"\"\n",
    "    html_content = {}\n",
    "    for result in search_results:\n",
    "        print(f\"Parsing {result}\")\n",
    "\n",
    "        # Check if web scraping is allowed by robots.txt\n",
    "        if can_scrape(result):\n",
    "            # Send an HTTP GET request to the website\n",
    "            try:\n",
    "                response = requests.get(result)\n",
    "            except ConnectionError:\n",
    "                print(\"Failed to retrieve the webpage.\")\n",
    "\n",
    "            # Check if the request was successful\n",
    "            if response.status_code == 200:\n",
    "                html_content[result] = response.text\n",
    "\n",
    "            else:\n",
    "                print(\"Failed to retrieve the webpage.\")\n",
    "        else:\n",
    "            print(\"Web scraping is not allowed by robots.txt.\")\n",
    "    return html_content\n",
    "\n",
    "\n",
    "def extract_html_text(html_content):\n",
    "    \"\"\"\n",
    "    Parse the webpage content with Beautiful Soup\n",
    "    \"\"\"\n",
    "    html_text = {}\n",
    "    for website, content in html_content.items():\n",
    "        html_text[website] = BeautifulSoup(content, \"html.parser\").get_text()\n",
    "    return html_text\n",
    "\n",
    "\n",
    "def search_fetch_text(google_query, num_results=3):\n",
    "    \"\"\"\n",
    "    Perform a google search and return the text content of the websites\n",
    "    for which scraping is not forbidden.\n",
    "    \"\"\"\n",
    "    search_results = list(search(google_query, num_results=num_results))\n",
    "    html_content = fetch_html_content(search_results)\n",
    "    return extract_html_text(html_content)\n",
    "\n",
    "\n",
    "def get_prompt_list_targets(target, website, website_text):\n",
    "    return (\n",
    "        f\"Extract a list of {target} from the text content of {website}. \"\n",
    "        f\"Use `{target}` as key in the json. \"\n",
    "        f\"If the content is not about {target}, return an empty list. \"\n",
    "        f\"Here's the text content extracted from html: ```{website_text}```\"\n",
    "    )\n",
    "\n",
    "\n",
    "def get_prompt_parsing_assistant():\n",
    "    return \"You are a parsing assistant, returning answers in json format.\"\n",
    "\n",
    "\n",
    "def get_prompt_extract_fields(fields, website, website_text):\n",
    "    return (\n",
    "        f\"Extract the following fields from the text content of {website}: {fields}. \"\n",
    "        f\"Use the fields as key in the output json. If a field is not available, return null for that field.\"\n",
    "        f\"Here's the text content extracted from html: ```{website_text}```\"\n",
    "    )\n",
    "\n",
    "\n",
    "def run_gpt(prompt, client):\n",
    "    return client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": get_prompt_parsing_assistant()},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "def completion_to_json(completion):\n",
    "    \"\"\"\n",
    "    Convert completion to json.\n",
    "    Assumes that the message is json de-serializable.\n",
    "    \"\"\"\n",
    "    return json.loads(completion.choices[0].message.model_dump()[\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99137814-9491-49ab-b5df-ef9fd470ae79",
   "metadata": {},
   "source": [
    "## Scraping\n",
    "\n",
    "We perform the following steps:\n",
    "1. Fetch a website using the google query\n",
    "2. Create a list of `target` using GPT on the website above\n",
    "3. For each target, use the google search to fetch a corresponding website.\n",
    "4. Use GPT to look for `fields` in the website content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28199b5b-7284-4373-86ce-f2cb6145242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's instantiate an OpenAI client. \n",
    "# Instructions to setup the OpenAI key are available at: https://platform.openai.com/docs/quickstart/step-2-setup-your-api-key\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad2285d-8ae6-40f4-895a-ee9ab9a268e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch websites and their content using google_query\n",
    "html_text = search_fetch_text(google_query, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fff922-8cdb-474c-a2f3-42d7039897aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use only the first result for this example.\n",
    "website = next(iter(html_text))\n",
    "website_text = html_text[website]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa2df0f-02c6-42f7-af04-cf3b0d01839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fetch a list of targets\n",
    "prompt = get_prompt_list_targets(target,website,website_text)\n",
    "completion = run_gpt(prompt,client)\n",
    "json_list = completion_to_json(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34a1df9-98a8-4b3b-b0ac-11c42aac73cf",
   "metadata": {},
   "source": [
    "The following cell should output something like:\n",
    "```\n",
    "{'publishing companies': ['Betty Bossi',\n",
    "  'BirkhäuserD',\n",
    "  'Diogenes VerlagE',\n",
    "  'Editions Librisme',\n",
    "  'Edizioni Casagrande',\n",
    "  'JRP-Ringier',\n",
    "  'Manesse Verlag',\n",
    "  'NZZ Mediengruppe',\n",
    "  'Orell Füssli',\n",
    "  'RCL Benziger',\n",
    "  'Schwabe (publisher)',\n",
    "  'Skira (publisher)',\n",
    "  'Société typographique de Neuchâtel',\n",
    "  'Stämpfli (publisher)']}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a043cd-474d-4c95-8d84-a9482518ac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ce0698-0ec1-4fbf-9157-38acda331e7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# For each target in the list, fetch fields\n",
    "\n",
    "json_data = {}\n",
    "for target_name in json_list[target]:\n",
    "    print(f\"*** Processing {target_name} ***\")\n",
    "\n",
    "    # Fetch websites and their content\n",
    "    try:\n",
    "        html_text = search_fetch_text(f\"{target} {target_name}\", 3)\n",
    "    except OSError:\n",
    "        print(\"Fetching websites content failed.\")\n",
    "        continue\n",
    "        \n",
    "    if len(html_text) == 0:\n",
    "        print(f\"Skipping {target_name}, no websites.\")\n",
    "        continue\n",
    "        \n",
    "    # Let's use only the first website available\n",
    "    website = next(iter(html_text))\n",
    "    website_text = html_text[website]\n",
    "    \n",
    "    # Fetch fields\n",
    "    prompt = get_prompt_extract_fields(fields,website,website_text)\n",
    "    try:\n",
    "        completion = run_gpt(prompt,client)\n",
    "    except BadRequestError:\n",
    "        print(\"Bad request from openai\")\n",
    "        continue\n",
    "    output = completion_to_json(completion)\n",
    "\n",
    "    # Store fields\n",
    "    output[\"source\"] = website\n",
    "    json_data[target_name] = output\n",
    "\n",
    "    print(\"output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7736f43f-22ad-4e7d-b06b-3969093d5fea",
   "metadata": {},
   "source": [
    "The following cell should output something like:\n",
    "```\n",
    "{'Betty Bossi': {'genres': None,\n",
    "  'year founded': None,\n",
    "  'location': None,\n",
    "  'official website': None,\n",
    "  'short description': 'Swiss cookbook publisher',\n",
    "  'source': 'https://en.wikipedia.org/wiki/Betty_Bossi'},\n",
    " 'Diogenes VerlagE': {'genres': None,\n",
    "  'year founded': '1952',\n",
    "  'location': 'Zurich',\n",
    "  'official website': 'https://www.diogenes.ch/',\n",
    "  'short description': \"Diogenes is the largest independent fiction publisher in Europe, with international bestselling authors and a comprehensive collection of classics, art and cartoon volumes, and children's books.\",\n",
    "  'source': 'https://www.diogenes.ch/foreign-rights/about.html'},\n",
    " 'Editions Librisme': {'genres': None,\n",
    "  'year founded': '2005',\n",
    "  'location': None,\n",
    "  'official website': 'editions-librisme.com',\n",
    "  'short description': 'publishing association',\n",
    "  'source': 'https://en.wikipedia.org/wiki/Editions_Librisme'},\n",
    " 'Edizioni CasagrandeJ': {'genres': None,\n",
    "  'year founded': '1949',\n",
    "  'location': 'Bellinzona in Switzerland',\n",
    "  'official website': 'http://www.edizionicasagrande.com/',\n",
    "  'short description': 'Italian-language publisher, focused on the art and history of Italian Switzerland.',\n",
    "  'source': 'https://en.wikipedia.org/wiki/Edizioni_Casagrande'},\n",
    " 'JRP-RingierM': {'genres': 'Contemporary art magazines',\n",
    "  'year founded': '2004',\n",
    "  'location': 'Zurich',\n",
    "  'official website': 'jrp-editions.com',\n",
    "  'short description': 'Swiss publisher of high-quality books on contemporary art',\n",
    "  'source': 'https://en.wikipedia.org/wiki/JRP-Ringier'},\n",
    " 'NZZ MediengruppeO': {'genres': 'Media',\n",
    "  'year founded': '1780',\n",
    "  'location': 'Zurich',\n",
    "  'official website': 'www.nzzmediengruppe.ch',\n",
    "  'short description': 'Swiss media company',\n",
    "  'source': 'https://en.wikipedia.org/wiki/NZZ_Mediengruppe'},\n",
    " 'RCL BenzigerS': {'genres': None,\n",
    "  'year founded': '1792',\n",
    "  'location': 'Cincinnati, Ohio',\n",
    "  'official website': 'www.rclbenziger.com',\n",
    "  'short description': 'Catholic publishing house',\n",
    "  'source': 'https://en.wikipedia.org/wiki/RCL_Benziger'},\n",
    " 'Schwabe (publisher)': {'genres': None,\n",
    "  'year founded': '1488',\n",
    "  'location': 'Basel',\n",
    "  'official website': 'www.schwabe.ch',\n",
    "  'short description': 'Swiss printer and publisher',\n",
    "  'source': 'https://en.wikipedia.org/wiki/Schwabe_(publisher)'},\n",
    " 'Skira (publisher)': {'genres': None,\n",
    "  'year founded': None,\n",
    "  'location': None,\n",
    "  'official website': None,\n",
    "  'short description': 'A boundless brand',\n",
    "  'source': 'https://www.skira.net/en/skira-around-the-world/'},\n",
    " 'Stämpfli (publisher)': {'genres': None,\n",
    "  'year founded': '1599',\n",
    "  'location': 'Wölflistrasse 1, 3001 Bern, Switzerland',\n",
    "  'official website': 'www.staempfli.com/en',\n",
    "  'short description': 'Swiss printing and publishing house',\n",
    "  'source': 'https://en.wikipedia.org/wiki/St%C3%A4mpfli_(publisher)'}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3b9763-75a4-40f5-bd5e-0fad561eaa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f35b0fd-801d-4dbb-822e-80fe2a3134dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can e.g. convert the output to a dataframe and store it for later use.\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame(json_data).transpose().reset_index().rename(columns={\"index\": target})\n",
    "data.to_csv(f\"scraped_{target.replace(' ','_')}.csv\", index=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffadd4fb-bdee-407b-9bb6-7a4c90d98697",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-on-html",
   "language": "python",
   "name": "llm-on-html"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
